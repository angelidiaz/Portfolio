{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MNIST CNN Image Classification\n",
    "\n",
    "This Jupyter notebook implements a Convolutional Neural Network (CNN) for classifying handwritten digits from the MNIST dataset using PyTorch.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Data Loading & Preprocessing**\n",
    "   - Loads the MNIST dataset and splits it into training, validation, and test sets.\n",
    "   - Reshapes the image data into tensors with shape `(batch_size, 1, 28, 28)` to match the CNN input requirements.\n",
    "   - Normalizes and batches the data for efficient training.\n",
    "\n",
    "2. **Model Architecture**\n",
    "   - Implements a CNN using PyTorch's `nn.Sequential`, consisting of:\n",
    "     - Two convolutional layers with ReLU activation\n",
    "     - MaxPooling layers to reduce spatial size\n",
    "     - Dropout for regularization\n",
    "     - Fully connected (linear) layers for classification\n",
    "   - A custom `Flatten` module is used to reshape feature maps into flat vectors before feeding into the fully connected layers.\n",
    "\n",
    "3. **Training Procedure**\n",
    "   - Uses Stochastic Gradient Descent (SGD) with optional momentum and Nesterov acceleration.\n",
    "   - Trains the model over several epochs, printing the training and validation loss/accuracy at each epoch.\n",
    "\n",
    "4. **Evaluation**\n",
    "   - Evaluates the model's performance on a held-out test set and reports final accuracy and loss.\n",
    "\n",
    "\n",
    "For detailed information about SGD optimizer parameters and configuration options, refer to the [PyTorch SGD documentation](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)."
   ],
   "id": "a0b7d205c10de8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Library Dependencies\n",
    "\n",
    "The following libraries are essential for our analysis:"
   ],
   "id": "20308adab59ce58b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-25T20:35:04.366654Z",
     "start_time": "2025-04-25T20:35:04.362363Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from preprocessing_data import get_MNIST_data\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loading the data",
   "id": "1d675a90d4b20819"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T20:35:05.686142Z",
     "start_time": "2025-04-25T20:35:04.395717Z"
    }
   },
   "cell_type": "code",
   "source": "train_x, train_y, test_x, test_y = get_MNIST_data()",
   "id": "a63585149e13f4a5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**`Flatten`**\n",
    " A custom PyTorch layer that reshapes a 4D input tensor of shape `(batch_size, channels, height, width)` into a 2D tensor of shape `(batch_size, -1)` by flattening all dimensions except the batch size. This is useful when transitioning from convolutional layers to fully connected layers.\n"
   ],
   "id": "2723411bd4921aab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T20:35:05.726283Z",
     "start_time": "2025-04-25T20:35:05.720538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Flatten(nn.Module):\n",
    "    \"\"\"A custom PyTorch layer that flattens the input tensor to 1D.\n",
    "    \n",
    "    This layer transforms a multi-dimensional input tensor into a 2D tensor\n",
    "    where the first dimension is the batch size and the second dimension\n",
    "    contains all other dimensions flattened into one dimension.\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Flattened output tensor of shape (batch_size, -1)\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n"
   ],
   "id": "3d0b18e44fc8027e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**`batchify_data(x_data, y_data, batch_size)`**\n",
    "  Takes raw feature/label arrays and splits them into a list of PyTorch tensor batches of a given size. Drops remainder to ensure consistent batch sizes.\n"
   ],
   "id": "cec694f4fd5c64e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T20:35:05.819492Z",
     "start_time": "2025-04-25T20:35:05.782492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def batchify_data(x_data, y_data, batch_size):\n",
    "    \"\"\"Creates batches of data for mini-batch training with PyTorch.\n",
    "\n",
    "    This function takes input data and corresponding labels, converts them into NumPy arrays,\n",
    "    and creates batches of a specified size. The function ensures all batches are of equal size\n",
    "    by dropping any remaining samples that don't fit into a complete batch.\n",
    "\n",
    "    Args:\n",
    "        x_data (array-like): Input features/data points to be batched.\n",
    "                            Can be a list, NumPy array, or similar structure.\n",
    "        y_data (array-like): Labels/targets corresponding to x_data.\n",
    "                            Must have the same length as x_data.\n",
    "        batch_size (int): Number of samples per batch. Should be > 0.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, where each dictionary represents a batch\n",
    "                   containing:\n",
    "                   - 'x': torch.Tensor of shape (batch_size, *feature_dims) with dtype float32\n",
    "                   - 'y': torch.Tensor of shape (batch_size,) with dtype int64\n",
    "    \"\"\"\n",
    "    # Convert inputs to NumPy arrays for consistent handling\n",
    "    x_data = np.array(x_data)  # Ensure it's a single NumPy array\n",
    "    y_data = np.array(y_data)  # Ensure it's a single NumPy array\n",
    "    # Calculate number of complete batches (dropping remainder)\n",
    "    N = (len(x_data) // batch_size) * batch_size\n",
    "    batches = []\n",
    "    for i in range(0, N, batch_size):\n",
    "        batches.append({\n",
    "            'x': torch.tensor(x_data[i:i+batch_size], dtype=torch.float32),\n",
    "            'y': torch.tensor(y_data[i:i+batch_size], dtype=torch.long\n",
    "        )})\n",
    "    return batches"
   ],
   "id": "3affa918f613fdac",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**`compute_accuracy(predictions, y)`**\n",
    "  Computes classification accuracy by comparing model predictions with ground truth labels."
   ],
   "id": "904c5dbb189f5e49"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T20:35:05.952693Z",
     "start_time": "2025-04-25T20:35:05.943031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_accuracy(predictions, y):\n",
    "    \"\"\"\n",
    "    Computes the classification accuracy between predicted and true labels.\n",
    "\n",
    "    Calculates the accuracy as the mean of correct predictions divided by total predictions.\n",
    "    Both inputs are converted to NumPy arrays for computation.\n",
    "\n",
    "    Args:\n",
    "        predictions (torch.Tensor): Model predictions.\n",
    "                                  Shape: (batch_size,)\n",
    "                                  dtype: torch.long\n",
    "        y (torch.Tensor): Ground truth labels.\n",
    "                         Shape: (batch_size,)\n",
    "                         dtype: torch.long\n",
    "\n",
    "    Returns:\n",
    "        float: Classification accuracy as a decimal between 0.0 and 1.0.\n",
    "               - 1.0 means all predictions are correct\n",
    "               - 0.0 means all predictions are wrong\n",
    "    \"\"\"\n",
    "    return np.mean(np.equal(predictions.numpy(), y.numpy()))"
   ],
   "id": "dc680fd67074638a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**`run_epoch(batches, model, optimizer)`**\n",
    "  Performs one full pass (epoch) over the given dataset (training or validation). Computes average loss and accuracy.\n",
    "  - If model is in training mode (`model.train()`), it performs backpropagation and optimization.\n",
    "  - If in evaluation mode (`model.eval()`), it only computes predictions and loss without updating weights.\n"
   ],
   "id": "8abbd062770facbc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T20:35:06.056971Z",
     "start_time": "2025-04-25T20:35:06.006977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_epoch(batches, model, optimizer):\n",
    "    \"\"\"\n",
    "    Processes one epoch of data through the model in either training or evaluation mode.\n",
    "\n",
    "    This function handles both training and evaluation passes through the data:\n",
    "    - In training mode (model.training=True): performs backpropagation and parameter updates\n",
    "    - In evaluation mode (model.training=False): only computes forward pass and metrics\n",
    "\n",
    "    Args:\n",
    "        batches (list): List of dictionaries, each containing:\n",
    "                       - 'x': torch.Tensor of shape (batch_size, *feature_dims)\n",
    "                       - 'y': torch.Tensor of shape (batch_size,) with class labels\n",
    "        model (torch.nn.Module): The neural network model to train/evaluate\n",
    "        optimizer (torch.optim.Optimizer): The optimizer for updating model parameters\n",
    "                                         (used only in training mode)\n",
    "\n",
    "    Returns:\n",
    "        tuple: Contains:\n",
    "            - avg_loss (float): Mean cross-entropy loss across all batches\n",
    "            - avg_accuracy (float): Mean prediction accuracy across all batches\n",
    "\n",
    "    Note:\n",
    "        - In training mode, gradients are zeroed before each batch\n",
    "        - Cross-entropy loss is used for classification\n",
    "        - Predictions are made using argmax on model outputs\n",
    "        - Accuracy is computed using exact matches between predictions and labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize lists to store per-batch metrics\n",
    "    losses = []\n",
    "    batch_accuracies = []\n",
    "\n",
    "    # Check if we're in training mode\n",
    "    is_training = model.training\n",
    "\n",
    "    # Process each batch\n",
    "    for batch in batches:\n",
    "        # Extract features and labels\n",
    "        x, y = batch['x'], batch['y']\n",
    "\n",
    "        # Forward pass through the model\n",
    "        out = model(x)\n",
    "\n",
    "        # Compute predictions and accuracy\n",
    "        predictions = torch.argmax(out, dim=1) # dim=1 for class dimension\n",
    "        batch_accuracies.append(compute_accuracy(predictions, y))\n",
    "\n",
    "        # Compute classification loss\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        losses.append(loss.data.item())\n",
    "\n",
    "        # Perform optimization step if in training mode\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()    # Clear previous gradients\n",
    "            loss.backward()          # Compute gradients\n",
    "            optimizer.step()         # Update parameters\n",
    "\n",
    "    # Compute epoch-level metrics\n",
    "    avg_loss = np.mean(losses)\n",
    "    avg_accuracy = np.mean(batch_accuracies)\n",
    "    return avg_loss, avg_accuracy\n"
   ],
   "id": "63590af04e9755f7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**`train_model(train_data, dev_data, model, ...)`**\n",
    "\n",
    "  Main training loop:\n",
    "  - Iterates through a fixed number of epochs.\n",
    "  - Runs training and validation in each epoch using `run_epoch`.\n",
    "  - Saves the model after each epoch.\n",
    "  - Returns the final validation accuracy."
   ],
   "id": "ad2df323ee2989a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T20:35:06.112507Z",
     "start_time": "2025-04-25T20:35:06.076854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(train_data,\n",
    "                dev_data,\n",
    "                model,\n",
    "                lr=0.01,\n",
    "                momentum=0.9,\n",
    "                nesterov=False):\n",
    "    \"\"\"\n",
    "    Trains a PyTorch model using SGD optimization and validates its performance.\n",
    "\n",
    "    This function implements a training loop that:\n",
    "    1. Initializes an SGD optimizer with specified parameters\n",
    "    2. Trains for 10 epochs, evaluating after each epoch\n",
    "    3. Saves the model after each epoch\n",
    "    4. Returns the final validation accuracy\n",
    "\n",
    "    Args:\n",
    "        train_data (list): List of training data batches, where each batch is a dict\n",
    "                          containing 'x' (features) and 'y' (labels) tensors\n",
    "        dev_data (list): List of validation data batches, similar structure to train_data\n",
    "        model (torch.nn.Module): The PyTorch model to train\n",
    "        lr (float, optional): Learning rate for SGD optimizer. Defaults to 0.01\n",
    "        momentum (float, optional): Momentum factor for SGD optimizer. Defaults to 0.9\n",
    "        nesterov (bool, optional): Whether to use Nesterov momentum. Defaults to False\n",
    "\n",
    "    Returns:\n",
    "        float: The validation accuracy from the final epoch\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize SGD optimizer with specified parameters\n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr=lr,\n",
    "                                momentum=momentum,\n",
    "                                nesterov=nesterov)\n",
    "    # Train for 10 epochs\n",
    "    for epoch in range(1, 11):\n",
    "        print(\"-------------\\nEpoch {}:\\n\".format(epoch))\n",
    "\n",
    "\n",
    "        # Training phase\n",
    "        loss, acc = run_epoch(train_data, model.train(), optimizer)\n",
    "        print('Train loss: {:.6f} | Train accuracy: {:.6f}'.format(loss, acc))\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_acc = run_epoch(dev_data, model.eval(), optimizer)\n",
    "        print('Val loss:   {:.6f} | Val accuracy:   {:.6f}'.format(val_loss, val_acc))\n",
    "\n",
    "        # Save current model state\n",
    "        torch.save(model, 'mnist_model_fully_connected.pt')\n",
    "\n",
    "    return val_acc"
   ],
   "id": "aa6a8ab6ca97ef2b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**`main()`**\n",
    "  Coordinates the full training workflow:\n",
    "  - Loads and reshapes the MNIST dataset.\n",
    "  - Splits the data into training, validation, and test sets.\n",
    "  - Shuffles the training data to avoid training bias.\n",
    "  - Defines the CNN model using `nn.Sequential`.\n",
    "  - Trains the model with `train_model`.\n",
    "  - Evaluates final model performance on the test set using `run_epoch`."
   ],
   "id": "bfb33d1c0226d3b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T20:35:06.238177Z",
     "start_time": "2025-04-25T20:35:06.135009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function for training and evaluating a CNN model on the MNIST dataset.\n",
    "\n",
    "    Workflow:\n",
    "    1. Data Loading and Preprocessing:\n",
    "       - Loads MNIST data\n",
    "       - Reshapes data into proper image format (1x28x28)\n",
    "       - Splits data into train/validation/test sets\n",
    "       - Applies random shuffling to training data\n",
    "       - Creates batches for all datasets\n",
    "\n",
    "    2. Model Architecture:\n",
    "       Creates a CNN with the following structure:\n",
    "       - Conv2D(1→32, 3x3) → ReLU → MaxPool(2x2)\n",
    "       - Conv2D(32→64, 3x3) → ReLU → MaxPool(2x2)\n",
    "       - Flatten\n",
    "       - Linear(1600→128) → Dropout(0.5)\n",
    "       - Linear(128→10)\n",
    "\n",
    "    3. Training and Evaluation:\n",
    "       - Trains model using SGD with Nesterov momentum\n",
    "       - Evaluates on test set\n",
    "       - Prints final test loss and accuracy\n",
    "\n",
    "    Data Dimensions:\n",
    "        - Input images: (batch_size, 1, 28, 28)\n",
    "        - Output: 10 classes (digits 0-9)\n",
    "        - Batch size: 32\n",
    "\n",
    "    Note:\n",
    "        - Uses 10% of training data for validation\n",
    "        - Applies shuffling only to training data\n",
    "        - Test set remains unshuffled for reproducibility\n",
    "    \"\"\"\n",
    "\n",
    "    # Load and prepare MNIST dataset\n",
    "    num_classes = 10\n",
    "    X_train, y_train, X_test, y_test = get_MNIST_data()\n",
    "\n",
    "    # Reshape flat images into 4D format (batch_size, channels, height, width)\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], 1, 28, 28))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], 1, 28, 28))\n",
    "\n",
    "    # Create validation split (90% train, 10% validation)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train,\n",
    "                                                      y_train,\n",
    "                                                      test_size=0.1)\n",
    "    # Shuffle training data\n",
    "    permutation = np.array([i for i in range(len(X_train))])\n",
    "    np.random.shuffle(permutation)\n",
    "    X_train = [X_train[i] for i in permutation]\n",
    "    y_train = [y_train[i] for i in permutation]\n",
    "\n",
    "    # Create batched datasets\n",
    "    batch_size = 32\n",
    "    train_batches = batchify_data(X_train, y_train, batch_size)\n",
    "    dev_batches = batchify_data(X_val, y_val, batch_size)\n",
    "    test_batches = batchify_data(X_test, y_test, batch_size)\n",
    "\n",
    "    # Define CNN model architecture\n",
    "    model = nn.Sequential(\n",
    "    # First convolutional block\n",
    "    nn.Conv2d(1, 32, (3, 3)),     # Input: 28x28x1  → Output: 26x26x32\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d((2, 2)),         # Output: 13x13x32\n",
    "\n",
    "    # Second convolutional block\n",
    "    nn.Conv2d(32, 64, (3, 3)),    # Output: 11x11x64\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d((2, 2)),         # Output: 5x5x64\n",
    "\n",
    "    # Fully connected layers\n",
    "    Flatten(),                     # Output: 1600 (5*5*64)\n",
    "    nn.Linear(1600, 128),\n",
    "    nn.Dropout(0.5),              # Prevent overfitting\n",
    "    nn.Linear(128, num_classes)    # Output: 10 classes\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    train_model(train_batches, dev_batches, model, nesterov=True)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    loss, accuracy = run_epoch(test_batches, model.eval(), None)\n",
    "    print(\"Loss on test set:\" + str(loss) + \" Accuracy on test set: \" + str(accuracy))\n"
   ],
   "id": "df4ba4c6d4035eb7",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Results\n",
    "\n",
    "The model's performance metrics track progression across epochs, displaying training and validation losses alongside their respective accuracies for each iteration. The final evaluation reports comprehensive test set performance metrics, demonstrating the model's generalization capabilities on unseen data."
   ],
   "id": "43f0c8eb1cbfe255"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T20:45:38.327588Z",
     "start_time": "2025-04-25T20:35:06.262498Z"
    }
   },
   "cell_type": "code",
   "source": "main()",
   "id": "9b19a7f79825d65f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "Epoch 1:\n",
      "\n",
      "Train loss: 0.242198 | Train accuracy: 0.922922\n",
      "Val loss:   0.069764 | Val accuracy:   0.978108\n",
      "-------------\n",
      "Epoch 2:\n",
      "\n",
      "Train loss: 0.080397 | Train accuracy: 0.975752\n",
      "Val loss:   0.048263 | Val accuracy:   0.986297\n",
      "-------------\n",
      "Epoch 3:\n",
      "\n",
      "Train loss: 0.060442 | Train accuracy: 0.981476\n",
      "Val loss:   0.041506 | Val accuracy:   0.985963\n",
      "-------------\n",
      "Epoch 4:\n",
      "\n",
      "Train loss: 0.048583 | Train accuracy: 0.984903\n",
      "Val loss:   0.038915 | Val accuracy:   0.987467\n",
      "-------------\n",
      "Epoch 5:\n",
      "\n",
      "Train loss: 0.040266 | Train accuracy: 0.987496\n",
      "Val loss:   0.038104 | Val accuracy:   0.988636\n",
      "-------------\n",
      "Epoch 6:\n",
      "\n",
      "Train loss: 0.034403 | Train accuracy: 0.989460\n",
      "Val loss:   0.034159 | Val accuracy:   0.989806\n",
      "-------------\n",
      "Epoch 7:\n",
      "\n",
      "Train loss: 0.030224 | Train accuracy: 0.990164\n",
      "Val loss:   0.033432 | Val accuracy:   0.989472\n",
      "-------------\n",
      "Epoch 8:\n",
      "\n",
      "Train loss: 0.026691 | Train accuracy: 0.991497\n",
      "Val loss:   0.034987 | Val accuracy:   0.989305\n",
      "-------------\n",
      "Epoch 9:\n",
      "\n",
      "Train loss: 0.024998 | Train accuracy: 0.992442\n",
      "Val loss:   0.034175 | Val accuracy:   0.989639\n",
      "-------------\n",
      "Epoch 10:\n",
      "\n",
      "Train loss: 0.021511 | Train accuracy: 0.993202\n",
      "Val loss:   0.029613 | Val accuracy:   0.992146\n",
      "Loss on test set:0.027423009827737717 Accuracy on test set: 0.9916866987179487\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "## Model Performance Analysis\n",
    "\n",
    "The Convolutional Neural Network (CNN) demonstrates excellent learning progression:\n",
    "- Performance improves consistently across epochs\n",
    "- Achieved test accuracy exceeding 98%\n",
    "- Shows strong generalization capabilities on unseen data\n",
    "\n",
    "## Comparative Analysis\n",
    "\n",
    "When compared to the traditional classification models implemented in `mnist_classification_part1.ipynb`, the CNN architecture demonstrates superior performance:\n",
    "- Better accuracy metrics\n",
    "- More efficient feature extraction through convolutional layers\n",
    "\n",
    "This improvement in performance validates the choice of using CNNs for image classification tasks."
   ],
   "id": "1fb169c828bbc5ec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
